{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-02-vector-representation-of-documents.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiAnKtlKK-9H"
      },
      "source": [
        "# Pridobitev vektorskih predstavitev besedil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6PJCzZjlI9f"
      },
      "source": [
        "V tem zvezku predstavimo, kako lahko pridobimo vektorske predstavitve (vložitve) besed in dokumentov za analizo besedil."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSZ5JxEKKvLZ"
      },
      "source": [
        "Za začetek si preko API-ja pridobimo besedila zadnjih 100 predlogov vladi, ki vsebujejo vsaj 50 znakov (s tem pogojem poskrbimo, da po predprocesiranju ne dobimo praznega seznama pojavnic)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxGoFv_5rjgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762835fe-4a2f-432c-9ec1-5c5d7c8d4cad"
      },
      "source": [
        "from textsemantics.server_api import ServerAPI\n",
        "\n",
        "api = ServerAPI()\n",
        "datasets = api.list_datasets()\n",
        "metadata = api.get_metadata(datasets[2][0], sample_size=100, sampling_strategy='latest')\n",
        "\n",
        "texts = api.get_texts(urls=metadata['text'])\n",
        "texts = [text for text in texts if len(text) > 50]\n",
        "print(f'Število predlogov vladi: {len(texts)}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Število predlogov vladi: 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIyjtMDbK4NN"
      },
      "source": [
        "Dobili smo 99 dokumentov. Zdaj lahko dokumente predprocesiramo tako, da iz njih odstranimo končnice, jih pretvorimo v seznam pojavnic, odstranimo prazne besede in lematiziramo preostale pojavnice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "WmHmUeH6sa6M",
        "outputId": "c94dcf68-ce73-4433-d80f-0a1b5b431492"
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from lemmagen.lemmatizer import Lemmatizer\n",
        "from lemmagen import DICTIONARY_SLOVENE\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def preprocess(corpus):\n",
        "    stop_words = set(stopwords.words('slovene'))\n",
        "    tokenizer = RegexpTokenizer(\"\\w+\")\n",
        "    lemmatizer = Lemmatizer(dictionary=DICTIONARY_SLOVENE)\n",
        "    \n",
        "    preprocessed = list()\n",
        "    for text in corpus:\n",
        "        text = text.translate(text.maketrans('', '', string.punctuation))\n",
        "        tokens = tokenizer.tokenize(text.lower())\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words \n",
        "                  and len(token) > 2 and not token.isnumeric()]\n",
        "        preprocessed.append(tokens)\n",
        "        \n",
        "    return preprocessed\n",
        "\n",
        "tokens_list = preprocess(texts)\n",
        "\n",
        "md_string = '### Prvih 10 pojavnic v prvem dokumentu\\n'\n",
        "for tok in tokens_list[0][:10]:\n",
        "    md_string += f\"- {tok}\\n\"\n",
        "display(Markdown(md_string))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Prvih 10 pojavnic v prvem dokumentu\n- meniti\n- slovenija\n- obdavčiti\n- gospodinjstvo\n- podjetje\n- obdavčiti\n- verski\n- institucija\n- obdavčiti\n- verski\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zVj8fFLLCMs"
      },
      "source": [
        "Sedaj lahko vsak dokument predstavimo kot vektor. Vektorje bomo dobili z uporabo vreče besed, kjer vsak atribut predstavlja eno besedo v slovarju, vsaka vrstica pa en dokument. Tabela predstavi število pojavitev posamezne besede za posamezen dokument. Tabelo lahko prilagodimo tako, da upoštevamo pogostost besed - manj pogoste, a pomembne besede bodo imele višjo vrednost kot take, ki so vseprisotne. Poleg tega bomo dokumente predstavili z uporabo modela fastText, ki temelji na nevronskih mrežah in je prednaučen na velikem korpusu dokumentov. V osnovi je fastText učen, da besede predstavi z nizkodimenzionalnimi vektorji, vendar lahko vektorje dokumentov dobimo s povprečenjem vektorjev besed, ki se v dokumentu nahajajo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3ZvZA5GLJGM",
        "outputId": "e0c770fc-74dc-44ce-88e9-3b8b4157d9c1"
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def vectorize(tokens_list, emb_type='fasttext'):\n",
        "    joined_texts = [' '.join(tokens) for tokens in tokens_list]\n",
        "\n",
        "    if emb_type=='fasttext':\n",
        "        embedder = DocumentPoolEmbeddings([WordEmbeddings('sl')],\n",
        "                                          pooling='mean')\n",
        "        X = list()\n",
        "        for i, doc in enumerate(joined_texts):\n",
        "            sent = Sentence(doc)\n",
        "            embedder.embed(sent)\n",
        "            X.append(sent.embedding.cpu().detach().numpy())\n",
        "        return np.array(X)\n",
        "    elif emb_type == 'tfidf':\n",
        "        return TfidfVectorizer().fit_transform(joined_texts)   \n",
        "    return None\n",
        "\n",
        "ft = vectorize(tokens_list, emb_type='fasttext')\n",
        "tfidf = vectorize(tokens_list, emb_type='tfidf')\n",
        "\n",
        "print(f'Matrika fastText: {ft.shape[0]} vrstic, {ft.shape[1]} stolpcev')\n",
        "print(f'Matrika vreče besed (tf-idf): {tfidf.shape[0]} vrstic, {tfidf.shape[1]} stolpcev')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrika fastText: 99 vrstic, 300 stolpcev\n",
            "Matrika vreče besed (tf-idf): 99 vrstic, 2270 stolpcev\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LlKfL5vmWTl"
      },
      "source": [
        "Za vložitve tf-idf smo dobili 99 x 2270 matriko, za vložitve fastText pa 99 x 300 matriko. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dvi3h_JLwHe"
      },
      "source": [
        "Dobljene vektorje si shranimo v datoteko, da bi jih lahko uporabljali v nadaljnjih primerih."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg6OS-RDLvXZ"
      },
      "source": [
        "import os\n",
        "from scipy.sparse import save_npz\n",
        "\n",
        "def save_data(ft, tfidf):\n",
        "    word_embs = list()\n",
        "    embedder = WordEmbeddings('sl')\n",
        "\n",
        "    for word in ['šola', 'počitnice', 'semafor', 'tehnologija']:\n",
        "        sent = Sentence(word)\n",
        "        embedder.embed(sent)\n",
        "        vec = sent.tokens[0].embedding.cpu().detach().numpy()\n",
        "        word_embs.append(vec)\n",
        "    word_embs = np.array(word_embs)\n",
        "\n",
        "    try:\n",
        "        os.mkdir('data')\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "    np.save('data/ft.npy', ft)\n",
        "    save_npz('data/tfidf.npz', tfidf)\n",
        "    np.save('data/words.npy', word_embs)\n",
        "\n",
        "save_data(ft, tfidf)"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}