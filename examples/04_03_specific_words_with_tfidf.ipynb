{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGwuELZZUfED"
   },
   "source": [
    "# Iskanje besed, specifičnih za dokumente z uporabo transformacije TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmEW2fr2Up_2"
   },
   "source": [
    "Tokrat bomo poskusili specifične besede v dokumentu določiti z uporabo transformacije TF-IDF, ki uteži besede glede na njihovo pogostost v besedilu. Besede, ki močno zaznamujejo manjšo množico dokumentov, bodo tako imele večjo težo kot take, ki so vseprisotne v korpusu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeLVMjMLWVhR"
   },
   "source": [
    "Iz seznama predlogov vladi izberemo zadnjih 100 dokumentov, jih predobledamo na enak način kot v prejšnjih primerih in za vsak dokument izračunamo najbolj specifične besede. Uporabili bomo utežitev TF-IDF in poiskali nekaj najvišje uteženih besed v množici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E7oBUANqIGzm"
   },
   "outputs": [],
   "source": [
    "from textsemantics.server_api import ServerAPI\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from lemmagen.lemmatizer import Lemmatizer\n",
    "from lemmagen import DICTIONARY_SLOVENE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def preprocess(corpus):\n",
    "    stop_words = set(stopwords.words('slovene'))\n",
    "    tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "    lemmatizer = Lemmatizer(dictionary=DICTIONARY_SLOVENE)\n",
    "    \n",
    "    preprocessed = list()\n",
    "    for text in corpus:\n",
    "        text = text.translate(text.maketrans('', '', string.punctuation))\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words \n",
    "                  and len(token) > 2 and not token.isnumeric()]\n",
    "        preprocessed.append(tokens)\n",
    "        \n",
    "    return preprocessed\n",
    "\n",
    "api = ServerAPI()\n",
    "datasets = api.list_datasets()\n",
    "metadata = api.get_metadata(datasets[2][0], sample_size=100, sampling_strategy='latest')\n",
    "\n",
    "texts = api.get_texts(urls=metadata['text'])\n",
    "texts = [text for text in texts if len(text) > 50]\n",
    "\n",
    "tokens_list = preprocess(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dLcquNeRIl0a"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "joined_texts = [' '.join(tokens) for tokens in tokens_list]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(joined_texts)\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tfidf_words(document_id, tfidf, feature_names, n_words=5):\n",
    "    n_words = n_words if n_words and n_words <= len(feature_names) else len(feature_names)\n",
    "    feature_index = tfidf[document_id,:].nonzero()[1]\n",
    "    features = [(feature_names[i], tfidf[document_id, i]) for i in feature_index]\n",
    "    return sorted(features, key=lambda tup: tup[1], reverse=True)[:n_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nato izberemo, koliko najvišje rangiranih besed nas zanima, ter kateri dokument bomo opazovali. Izbrali smo najbolj zanimivih pet be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-BQewUSqXzAi"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_document_and_specific_words(doc_ind): \n",
    "    top_words = find_tfidf_words(doc_ind, X, feature_names, n_words=num_of_words)\n",
    "    s = f\"## {metadata.iloc[doc_ind]['title']}\\n {texts[doc_ind]}\\n\"\n",
    "    s += f\"\\n### {num_of_words} najvišje uteženih besed z metodo TF-IDF:\\n\"\n",
    "    s += \"|Beseda|Utež|\\n\"\n",
    "    s += \"|---|---|\\n\"\n",
    "    for word, value in top_words:\n",
    "        s += f'|{word:<15}|{value:.2f}|\\n'\n",
    "\n",
    "    display(Markdown(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwJ0sr9BaOJN"
   },
   "source": [
    "Poglejmo dva dokumenta in 5 najbolj specifičnih besed, ki jih najde utežitev TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "NZUK3dHBX_BM",
    "outputId": "9504042e-8fd1-4f5c-cf93-d46e20431699",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Sprememba stopnje DDV za stanovanjske nepremičnine\n",
       " Trenutna obdavčitev je 9,5% za stanovanja do 120m2 in hiše do 250m2. Če površina presega cenzus, se na celotno ceno obračuna DDV po stopnji 22%. Posledično je stanovanje površine 121m2 več kot dvakrat bolj obdavčeno, kot stanovanje površine 119m2, kar je absurdno.\n",
       "Predlagam, da se obdavčitev spremeni tako, da je površina do 120m2 stanovanja oz. 250m2 hiše vedno obdavčena po 9,5%. Površina, ki presega cenzus, pa naj bo višje obdavčena.\n",
       "\n",
       "### 5 najvišje uteženih besed z metodo TF-IDF:\n",
       "|Beseda|Utež|\n",
       "|---|---|\n",
       "|površina       |0.56|\n",
       "|stanovanje     |0.39|\n",
       "|obdavčen       |0.34|\n",
       "|cenzus         |0.22|\n",
       "|250m2          |0.22|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_document_and_specific_words(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Videvanja z partnerjem z tujine med epidemijo.\n",
       " Predlagam, da se kot izjema za prehajanje regije, občine ali državne meje, uvede tudi vzdrževanje stikov z partnerji iz tujine. Sam, kot tudi drugi smo namreč zaradi tega prizadeti, saj nas veliko živi ob meji s Hrvaško na primer, 15 min vožnje stran eden od drugega, a zaradi restriktivnih srečanje ni možno, čez Kolpo si pa lahko mahamo. Je bil 1. Val dovolj, da se nisem mogel videti s punco, katere tudi skoraj nisem prepoznal, po 2 mesecih razdvojenosti. Lp\n",
       "\n",
       "### 5 najvišje uteženih besed z metodo TF-IDF:\n",
       "|Beseda|Utež|\n",
       "|---|---|\n",
       "|meja           |0.32|\n",
       "|razdvojenost   |0.20|\n",
       "|punca          |0.20|\n",
       "|mahati         |0.20|\n",
       "|kolpa          |0.20|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_document_and_specific_words(13)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02-05-specific-words-with-embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
