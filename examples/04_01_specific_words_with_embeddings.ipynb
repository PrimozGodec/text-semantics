{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-05-specific-words-with-embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGwuELZZUfED"
      },
      "source": [
        "# Iskanje besed specifičnih za dokumente z uporabo vložitev fastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmEW2fr2Up_2"
      },
      "source": [
        "Kot je že prikazano v enem od prejšnjih primerov, je z uporabo vložitev fastText zelo enostavno računati razdalje in podobnosti med dokumenti in besedami. Takrat smo pokazali, kako za eden dokument lahko določimo specifične besede z iskanjem tistih, ki so najbolj podobne dokumentu. Vendar pa je lahko beseda zelo podobna tudi ostalim dokumentom in takrat bi rekli, da taka beseda ne bo specifična za kateri koli dokument. V takem primeru lahko pri računanju specifične besede za določeni dokument upoštevamo tudi njeno podobnost z drugimi dokumenti. Tako želimo dobiti tiste besede, ki so čim bližje ciljnemu dokumentu in čim bolj daleč stran od vseh ostalih."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeLVMjMLWVhR"
      },
      "source": [
        "Iz seznama predlogov vladi si izberimo zadnjih 100 dokumentov, jih predobledajmo na isti način kot v prejšnjih primerih in za vsak dokument izračunajmo najbolj specifične besede. Uporabili bomo dva pristopa z eno ključno razliko - pri prvem pristopu je kandidat za specifično besedo posameznega dokumenta vsaka beseda, ki se nahaja v korpusu, pri drugem so pa kandidati le besede, ki se nahajajo v ciljenm dokumentu. Pričakujemo, da bodo pri prvem pristopu boljši rezultati, vendar je tudi časovna zahtevnost računanja večja."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7oBUANqIGzm"
      },
      "source": [
        "from textsemantics.server_api import ServerAPI\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from lemmagen.lemmatizer import Lemmatizer\n",
        "from lemmagen import DICTIONARY_SLOVENE\n",
        "from flair.data import Sentence\n",
        "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "def preprocess(corpus):\n",
        "    stop_words = set(stopwords.words('slovene'))\n",
        "    tokenizer = RegexpTokenizer(\"\\w+\")\n",
        "    lemmatizer = Lemmatizer(dictionary=DICTIONARY_SLOVENE)\n",
        "    \n",
        "    preprocessed = list()\n",
        "    for text in corpus:\n",
        "        text = text.translate(text.maketrans('', '', string.punctuation))\n",
        "        tokens = tokenizer.tokenize(text.lower())\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words \n",
        "                  and len(token) > 2 and not token.isnumeric()]\n",
        "        preprocessed.append(tokens)\n",
        "        \n",
        "    return preprocessed\n",
        "\n",
        "api = ServerAPI()\n",
        "datasets = api.list_datasets()\n",
        "metadata = api.get_metadata(datasets[2][0], sample_size=100, sampling_strategy='latest')\n",
        "\n",
        "texts = api.get_texts(urls=metadata['text'])\n",
        "texts = [text for text in texts if len(text) > 50]\n",
        "\n",
        "tokens_list = preprocess(texts)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLcquNeRIl0a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def prepare_data(tokens_list):\n",
        "    embedder = WordEmbeddings('sl')\n",
        "    words = list()\n",
        "    word_embs = list()\n",
        "    doc_embs = list()\n",
        "    word2doc = dict()\n",
        "    doc2word = list()\n",
        "\n",
        "    for i, tokens in enumerate(tokens_list):\n",
        "        sent = Sentence(\" \".join(tokens))\n",
        "        embedder.embed(sent)\n",
        "        doc_emb = np.zeros(embedder.embedding_length)\n",
        "        doc2word.append(set())\n",
        "        for token in sent.tokens:\n",
        "            if token.text not in word2doc:\n",
        "                word2doc[token.text] = set()\n",
        "            word2doc[token.text].add(i)\n",
        "            doc2word[i].add(token.text)\n",
        "\n",
        "            emb = token.embedding.cpu().detach().numpy()\n",
        "            doc_emb += emb / len(tokens)\n",
        "            if token.text not in words:\n",
        "                words.append(token.text)\n",
        "                word_embs.append(emb)\n",
        "        doc_embs.append(doc_emb)\n",
        "\n",
        "    doc_embs = np.array(doc_embs)\n",
        "    word_embs = np.array(word_embs)\n",
        "\n",
        "    return doc_embs, words, word_embs, word2doc, doc2word\n",
        "\n",
        "\n",
        "def cos_sim(x, y):\n",
        "    return x.dot(y) / np.linalg.norm(x) / np.linalg.norm(y)\n",
        "\n",
        "\n",
        "def find_specific_words(doc_embs, words, word_embs):\n",
        "\n",
        "    # compute distances\n",
        "    distances = np.zeros((word_embs.shape[0], doc_embs.shape[0]))\n",
        "    for i in range(word_embs.shape[0]):\n",
        "        for j in range(doc_embs.shape[0]):\n",
        "            distances[i, j] = 1 - cos_sim(word_embs[i, :], doc_embs[j, :])\n",
        "    \n",
        "    # compute scores\n",
        "    doc_desc = dict()\n",
        "    for j in range(doc_embs.shape[0]):\n",
        "        scores = np.zeros(word_embs.shape[0])\n",
        "        for i in range(word_embs.shape[0]):\n",
        "            mask = np.full(doc_embs.shape[0], fill_value=True)\n",
        "            mask[j] = False\n",
        "            scores[i] = distances[i, j] - np.mean(distances[i, mask])\n",
        "        \n",
        "        idx = np.argsort(scores)\n",
        "        doc_desc[j] = [words[w] for w in idx]\n",
        "\n",
        "    return doc_desc\n",
        "\n",
        "\n",
        "def find_specific_words2(doc_embs, words, word_embs, word2doc, doc2word):\n",
        "\n",
        "    word2emb = dict(zip(words, word_embs))\n",
        "    word2ind = dict(zip(words, range(len(words))))\n",
        "    distances = dict()\n",
        "    for i in range(word_embs.shape[0]):\n",
        "        word = words[i]\n",
        "        for j in word2doc[word]:\n",
        "            distances[i, j] = 1 - cos_sim(word_embs[i, :], doc_embs[j, :])\n",
        "\n",
        "\n",
        "    doc_desc = list()\n",
        "    for j in range(doc_embs.shape[0]):\n",
        "        scores = np.zeros(len(doc2word[j]))\n",
        "        ind2word = dict(zip(list(range(len(doc2word[j]))), list(doc2word[j])))\n",
        "        for k, word in enumerate(doc2word[j]):\n",
        "            # current document and a single word\n",
        "            i = word2ind[word]\n",
        "            sum_of_distances = sum([distances[i, x] for x in word2doc[word]])\n",
        "            mean_distance = (sum_of_distances - distances[i, j]) / (doc_embs.shape[0] - 1)\n",
        "            scores[k] = distances[i, j] - mean_distance\n",
        "        idx = np.argsort(scores)\n",
        "        doc_desc.append([ind2word[x] for x in idx])\n",
        "\n",
        "    return doc_desc \n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRy2jMkNSfPK",
        "outputId": "ded2df0a-a7f1-4413-9caa-9f28ffaebe3c"
      },
      "source": [
        "doc_embs, words, word_embs, word2doc, doc2word = prepare_data(tokens_list)\n",
        "doc_desc = find_specific_words(doc_embs, words, word_embs)\n",
        "doc_desc2 = find_specific_words2(doc_embs, words, word_embs, word2doc, doc2word)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BQewUSqXzAi"
      },
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def display_document_and_specific_words(doc_ind, n_words, doc_desc, doc_desc2): \n",
        "    s = f\"## Dokument\\n {texts[doc_ind]}\\n\"\n",
        "    s += f\"\\n### {n_words} najbolj specifičnih besed odkritih s prvo metodo:\\n\"\n",
        "    for w in doc_desc[doc_ind][:n_words]:\n",
        "        s += f'- {w}\\n'\n",
        "    s += f\"\\n### {n_words} najbolj specifičnih besed odkritih z drugo metodo:\\n\"\n",
        "    for w in doc_desc2[doc_ind][:n_words]:\n",
        "        s += f'- {w}\\n'\n",
        "\n",
        "    display(Markdown(s))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwJ0sr9BaOJN"
      },
      "source": [
        "Poglejmo eden dokument in 5 najbolj specifičnih besed, ki jih najdeta oba pristopa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "NZUK3dHBX_BM",
        "outputId": "9504042e-8fd1-4f5c-cf93-d46e20431699"
      },
      "source": [
        "display_document_and_specific_words(4, 5, doc_desc, doc_desc2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Dokument\n Trenutna obdavčitev je 9,5% za stanovanja do 120m2 in hiše do 250m2. Če površina presega cenzus, se na celotno ceno obračuna DDV po stopnji 22%. Posledično je stanovanje površine 121m2 več kot dvakrat bolj obdavčeno, kot stanovanje površine 119m2, kar je absurdno.\nPredlagam, da se obdavčitev spremeni tako, da je površina do 120m2 stanovanja oz. 250m2 hiše vedno obdavčena po 9,5%. Površina, ki presega cenzus, pa naj bo višje obdavčena.\n\n### 5 najbolj specifičnih besed odkritih s prvo metodo:\n- površina\n- obdavčen\n- obdavčitev\n- stanovanjski\n- višina\n\n### 5 najbolj specifičnih besed odkritih z drugo metodo:\n- predlagati\n- obdavčitev\n- obdavčen\n- površina\n- stanovanje\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbLRu5aXaEo5"
      },
      "source": [
        "Oba pristopa najdeta podobne besede, ki so specifične za dokument. Iz besed lahko sklepamo, o čem predlog vladi govori. Zdi se pa, da je tudi drugi pristop zelo dober ali pa celo boljši od prvega. Poglejmo si razliko na drugem primeru."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "rmN3HMFCTqwl",
        "outputId": "f68e61d1-d1cf-4409-8bc7-cf28fb21537c"
      },
      "source": [
        "display_document_and_specific_words(8, 5, doc_desc, doc_desc2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Dokument\n Pri klasičnem d.o.o. mora znašati osnovni kapital vsaj 7.500 evrov.\nPredlagam, da se osnovni kapital zniža, kajti to bi spodbudilo možnosti odprtja manjših novih in inovativnih podjetij, posebej v času epidemije. Odpira se veliko novih možnosti za poslovanje preko interneta, nove možnosti za prijave na evropske razpise in sodelovanja s tujino. Začetni vložek je prevelik za ustanovitev manjšega podjetja, zato predlagam, da se ta zmanjša. Posledično bi to tudi vplivalo na gospodarsko rast in zmanjševanje dela na črno.\nPrimer, na Hrvaškem so pred nekaj časa osnovni kapital znižali in znaša 20.000 HRK = 2645 eurov.\n\n\n### 5 najbolj specifičnih besed odkritih s prvo metodo:\n- kapital\n- povečati\n- rast\n- dobiček\n- priliv\n\n### 5 najbolj specifičnih besed odkritih z drugo metodo:\n- predlagati\n- osnoven\n- velik\n- zmanjšati\n- spodbuditi\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5kGLSEramz-"
      },
      "source": [
        "V tem primeru iz 5 najbolj specifičnih besed odkritih z drugim pristopom težko rečemo, o čem predlog vladi govori. Beseda kapital se nahaja šele na 15. mestu."
      ]
    }
  ]
}